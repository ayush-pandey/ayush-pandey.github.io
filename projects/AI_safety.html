<!DOCTYPE html>
<html lang="en">
<head>
	<title> Ayush Pandey </title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="archive, accent, , Ayush Pandey, jekyll">
    <meta name="author" content="">
    
    
    
    <meta name="description" content="">
    <link href='https://fonts.googleapis.com/css?family=Inconsolata:400,700' rel='stylesheet' type='text/css'>
    <link rel="alternate" type="application/rss+xml" title="Ayush Pandey RSS" href="/feed.xml" />
    <link rel="stylesheet" href="/css/main.css">
    
    
    <!-- Facebook Open Graph -->
    <meta name="og:description" content="">
    <meta name="og:title" content="Ayush Pandey">
    <meta name="og:url" content="/projects/AI_safety.html">
    <meta name="og:type" content="article">
    
    
    <!-- Twitter -->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Ayush Pandey">
    <meta name="twitter:description" content="">
    <meta name="twitter:url" content="/projects/AI_safety.html">
    
        <meta name="twitter:image" content="">
    
</head>
<body>
    <div class="wrapper">
        <div class="navbar container">
            <a id="author-name" class="alignable pull-left" href="/">Ayush Pandey</a>
            <ul class="alignable pull-right navbar-ul">
                
                    
                        <li class="alignable pull-right nav-list"><a href="/contact.html">Contact</a>
                    
                    
                        </li>

                        <li class="alignable pull-right nav-list"><a href="/teaching.html">Teaching & Mentoring</a>
                    
                    
                        /
                    
                        </li>
<!--                     
                        <li class="alignable pull-right nav-list"><a href="/projects.html">Projects</a>
                    
                    
                        /
                    
                        </li> -->
                
                    
                        <li class="alignable pull-right nav-list"><a href="/publication.html">Publications</a>
                    
                    
                        /
                    
                        </li>
                
                    
                        <li class="alignable pull-right nav-list"><a href="/blog.html">Blog</a>
                    
                    
                        /
                    
                        </li>
                
                    
                        <li class="alignable pull-right nav-list"><a target="_blank" href="/Ayush_Pandey_CV.pdf">CV</a>
                    
                    
                        /
                    
                        </li>
                
                    
                        <li class="alignable pull-right nav-list"><a href="/">About</a>
                    
                    
                        /
                    
                        </li>
                
            </ul>
        </div>
        <div style="clear:both"></div>
        <hr>
        
            <div class="container">
                <h2>Safety guarantees and robustness quantification of generative AI models</h2>

                    <p> Generative AI can enhance the performance of all kinds of systems by automating various manual tasks. However, such large language models are not ready for safety-critical systems yet. LLMs are known to “hallucinate” — AI provides incorrect responses that sound reasonable. Additionally, they are susceptible to attacks with prompt engineering. Consequently, public trust in AI is low as there are no guarantees on its performance/failure. In many AI applications, situations often arise where the AI model is confident in an incorrect/hazardous prediction. This is unacceptable when people's lives are at stake (such as in healthcare, aviation, or self-driving car applications). Building responsible, ethical, and trustworthy AI systems is a bottleneck in realizing the full potential of AI. The goal of this project is to utilize formal methods to characterize worst-case guarantees and the safety of AI models. We hope to integrate control theory and formal methods with AI to establish these safety guarantees.</p>

                <h3>Current participants</h3>
                <ol>
                    <li> None </li>
                </ol>
                <h3>Collaborators</h3>
                <ol>
                    <li> Gireeja Ranade, UC Berkeley</li>
                </ol>
                <hr/>
                <hr/>
                <h3>Selected references for this research area</h3>
                <ol>
                    <li>Elhage, Nelson, et al. "A mathematical framework for transformer circuits." Transformer Circuits Thread 1 (2021): 1.</li>
                    <li>Ji, Ziwei, et al. "Survey of hallucination in natural language generation." ACM Computing Surveys 55.12 (2023): 1-38.</li>
                    <li>Olsson, Catherine, et al. "In-context learning and induction heads." arXiv preprint arXiv:2209.11895 (2022).</li>
                    <li>Jha, Susmit, et al. "Dehallucinating large language models using formal methods guided iterative prompting." 2023 IEEE International Conference on Assured Autonomy (ICAA). IEEE, 2023.</li>
                    <li>Giraldo, Jairo, and Alvaro A. Cardenas. "A new metric to compare anomaly detection algorithms in cyber-physical systems." Proceedings of the 6th Annual Symposium on Hot Topics in the Science of Security. 2019.</li>
                    <li>Kumar, Aounon, et al. "Certifying llm safety against adversarial prompting." arXiv preprint arXiv:2309.02705 (2023).</li>
                    <li>Li, Zelong, et al. "Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents." arXiv preprint arXiv:2402.00798 (2024).</li>
                    <li>Jha, Susmit, et al. "Detecting adversarial examples using data manifolds." MILCOM 2018-2018 IEEE Military Communications Conference (MILCOM). IEEE, 2018.</li>
                </ol>
                <h3> Funding </h3>
                <p> This project is supported by the UC <a href="https://citris-uc.org/">CITRIS and the Banatao Research Institute</a> (Center for Information Technology Research in the Interest of Society and the Banatao Institute (CITRIS)) as a <a href="https://citris-uc.org/2023-seed-award-will-explore-safety-of-ai-in-air-traffic-control/">seed grant</a> and by the UC Merced Academic Senate.</p>
                <p>Last update: Mar 15, 2024. You can contribute to this page by creating a <a href="https://github.com/ayush-pandey/ayush-pandey.github.io/pulls">pull request on GitHub</a>.</p>
                </div>
        </div>
    </body>
</html>